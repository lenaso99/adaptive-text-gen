{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"...\"\n",
    "\n",
    "# load datasets\n",
    "eng_data = pd.read_excel(\"eng_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a70396",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "pipe = pipeline(\"text-generation\", model=model, trust_remote_code=True, token=hf_token)\n",
    "\n",
    "# generate responses with pipeline\n",
    "eng_responses = []\n",
    "for _, row in eng_data.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    prompt = f\"<INST>Given the following text, please generate a prompt that it could be a response for:\\n\\\"{text}\\\"</INST>\"\n",
    "\n",
    "    result = pipe(prompt, max_new_tokens=50)\n",
    "    generated = result.split(\"</INST>\")[-1].strip()\n",
    "\n",
    "    eng_responses.append(generated)\n",
    "eng_data[\"prompt\"] = eng_responses\n",
    "eng_train, eng_test = train_test_split(eng_data, test_size=0.2, random_state=42, stratify=eng_data[\"normalized_level\"])\n",
    "eng_data.to_excel(\"eng_data_with_prompt.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a775ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer_name_or_path = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9844d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model & tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"cuda\", token=hf_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb5a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
