import os
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import openai
import pandas as pd
import textstat
from dotenv import load_dotenv
from openai import OpenAI

def gpt_ranking(prompt, responses, model="gpt-4o-mini"):
    '''
    Function to rank the different responses generated by the model.
    It does evaluation in the following ways:
        - using a gpt model to rank the responses in difficulty, from most elementary to most advanced
        - using a gpt model for notes & explanations for the ranking
        - using existing scores to rate reach of the responses, for qualitative analysis
        - if specified, using a gpt model to compare the generated response(s) to the texts provided by the user,
            deciding whether or not the model successfully adapted to the user's language use

    As parameters, it takes:
        - prompt [str]: the prompt used to generate the responses
        - responses [list]: the list of the 6 responses to be evaluated

    Returns:
        - pd.DataFrame object: containing evaluation results
    '''
    if len(responses) != 6:
        raise ValueError("You must provide exactly 6 responses for ranking.")

    labels = ["A", "B", "C", "D", "E", "F"]
    formatted_responses = "\n".join([f"{labels[i]}:\n{responses[i]}\n" for i in range(6)])

    gpt_prompt = f"""You will be given a list of 6 different responses to a prompt. Your task is to rank these responses on the language used, from most elementary (1) to most advanced (6).\n
                    PROMPT:
                    {prompt}

                    RESPONSES:
                    {formatted_responses}

                    Please provide a ranking of these responses from 1 to 6, and provide a short explanation for each ranking. Focus solely on the language used, not on the informativity of the response. This includes things such as the number of difficult words in a sentence, or how complex the sentence structure is.
                    Format your response following this example format, where A-F are the labels of the responses and 1-6 are your ratings:
                    
                    RANKING:
                    A: 1    <explanation>
                    B: 2    <explanation>
                    C: 3    <explanation>
                    D: 4    <explanation>
                    E: 5    <explanation>
                    F: 6    <explanation>
                    """
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful language teacher assistent that helps evaluating texts based on their complexity."},
            {"role": "user", "content": gpt_prompt},
        ]
    )
    reply = response.choices[0].message.content.strip()

    # parsing the response:
    rankings = []
    for line in reply.splitlines():
        if any(line.startswith(f"{label}:") for label in labels):
            try:
                label, rest = line.split(":", 1)
                rank, explanation = rest.strip().split(" ", 1)
                rankings.append({
                    "label": label.strip(),
                    "response": responses[labels.index(label.strip())], 
                    "rank": int(rank.strip()),
                    "explanation": explanation.strip()
                    })
            except:
                print(f"Could not parse line: {line}")

    return pd.DataFrame(rankings).sort_values("rank")

def score_calc(dataframe, scores=["fkgl"]):
    '''
    Function to calculate readability scores for the responses, e.g. FKGL.
    
    As parameters, it takes:
        - dataframe [pd.DataFrame]: the dataframe containing the responses to be evaluated
        - scores [list]: list of scores to be calculated (default: ["fkgl"])
        
    Returns:
        - pd.DataFrame object: containing the calculated scores
    '''
    df = dataframe.copy()
    df["fkgl"] = df["response"].apply(textstat.flesch_kincaid_grade)
    df["fre"] = df["response"].apply(textstat.flesch_reading_ease)
    return df

if __name__ == "__main__":
    load_dotenv("../.env")
    openai_api_key = os.getenv("OPENAI_API_KEY")

    client = OpenAI(api_key=openai_api_key)

    # loading results dataset to evaluate
    '''
    print("Enter path to file for evaluation (Excel file):")
    user_input = input(">>> Path to file: ")
    try:
        if user_input.endswith(".xlsx"):
            df = pd.read_excel(user_input)
        elif user_input.endswith(".csv"):
            df = pd.read_csv(user_input)
        else:
            raise ValueError("File must be either .xlsx or .csv")
    except Exception as e:
        print(f"Error reading file: {e}")
        exit(1)
    '''

    
    df = pd.DataFrame({
        "prompt": ["What is photosynthesis?"] * 6,
        "level": ["elementary 1", "elementary 2", "intermediate 1", "intermediate 2", "advanced 1", "advanced 2"],
        "response": [
            "Plants make food from sunlight.",
            "Plants use sunlight to make their own food.",
            "Photosynthesis is a process plants use to turn sunlight into energy.",
            "Photosynthesis converts light energy into chemical energy stored in glucose.",
            "Photosynthesis involves complex reactions using chlorophyll and sunlight to produce sugars.",
            "In photosynthesis, plants convert light energy into chemical energy through a series of enzymatic reactions."
        ]
    })

    # Group the responses by prompt
    results = []
    for prompt, group in df.groupby("prompt"):
        group_sorted = group.sort_values("level")  # make sure ordering is consistent
        responses = group_sorted["response"].tolist()
        levels = group_sorted["level"].tolist()

        ranked_df = gpt_ranking(prompt, responses)

        # Add original level by matching each response
        level_map = dict(zip(responses, levels))
        ranked_df["level"] = ranked_df["response"].map(level_map)

        # Add prompt info back
        ranked_df["prompt"] = prompt
        results.append(ranked_df)


    # Combine into a single DataFrame
    final_df = pd.concat(results, ignore_index=True)

    # for dataframe, calculate scores
    final_df = score_calc(final_df)



    # Save to excel
    final_df.to_excel("results/ranked_gpt_evaluation.xlsx", index=False)
